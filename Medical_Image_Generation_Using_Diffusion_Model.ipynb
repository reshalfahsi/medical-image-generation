{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gXHHMdcFdElR",
        "KZ7tR9GN_A97",
        "eY3g1ipM9dKL",
        "v32HpTp99gCc",
        "WHq5wFak9maH",
        "MunmaGXF1Zvp",
        "OUP90Trjij2r",
        "uWcPMvglioOY",
        "tn_nP8Uqmi6b",
        "boRJQCOJdFXa",
        "rx7WK_KxdFhM",
        "9U0lYLLUR23C",
        "ym6UVoVoR6NE"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reshalfahsi/medical-image-generation/blob/master/Medical_Image_Generation_Using_Diffusion_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Medical Image Generation Using Diffusion Model**"
      ],
      "metadata": {
        "id": "nxLWNsrydEIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Important Libraries**"
      ],
      "metadata": {
        "id": "gXHHMdcFdElR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --no-cache-dir lightning torchmetrics medmnist torch-fidelity"
      ],
      "metadata": {
        "id": "g8PrGK1xdQNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge_Yk5-mc9b9"
      },
      "outputs": [],
      "source": [
        "import lightning as L\n",
        "from lightning.pytorch import Trainer, seed_everything\n",
        "from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger\n",
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, Resize\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from medmnist.dataset import MedMNIST\n",
        "from medmnist.info import INFO\n",
        "from medmnist.utils import montage2d\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import math\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['axes.facecolor'] = 'lightgray'\n",
        "plt.rcParams['mathtext.fontset'] = 'cm'\n",
        "plt.rcParams['font.family'] = 'STIXGeneral'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configuration**"
      ],
      "metadata": {
        "id": "KZ7tR9GN_A97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_EPOCH = 10\n",
        "DIFFUSION_STEP = 1000\n",
        "BATCH_SIZE = 20\n",
        "LR = 3e-4\n",
        "CHECKPOINT_DIR = os.getcwd()\n",
        "FLAG = \"bloodmnist\"\n",
        "IMAGE_SIZE = 48\n",
        "SCALE_DOWN = 2\n",
        "N_CHANNEL = INFO[FLAG]['n_channels']\n",
        "BETA_START = 1e-4\n",
        "BETA_END = 2e-2"
      ],
      "metadata": {
        "id": "WCjIc6r__B8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset**"
      ],
      "metadata": {
        "id": "PrhyQrbmdFCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Configuration**"
      ],
      "metadata": {
        "id": "eY3g1ipM9dKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_SEED = int(np.random.randint(2147483647))\n",
        "print(f\"Random seed: {DATA_SEED}\")"
      ],
      "metadata": {
        "id": "X84-kMY0CThR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Utils**"
      ],
      "metadata": {
        "id": "v32HpTp99gCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_transform = Compose(\n",
        "    [\n",
        "        Resize(IMAGE_SIZE),\n",
        "        ToTensor(),\n",
        "        Lambda(lambda x: (x * 2) - 1),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "cJPUtdXMgXzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MedMNIST2D(MedMNIST):\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        return: (without transform/target_transofrm)\n",
        "            img: PIL.Image\n",
        "        \"\"\"\n",
        "\n",
        "        if self.split in [\"test\", \"val\"]:\n",
        "            index = random.randint(0, len(self.imgs) - 1)\n",
        "\n",
        "        img = self.imgs[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.as_rgb:\n",
        "            img = img.convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def cut(self):\n",
        "        CUT_INDEX = len(self.imgs) // 100 * 100\n",
        "        self.imgs = self.imgs[:CUT_INDEX]\n",
        "        self.info[\"n_samples\"][self.split] = len(self.imgs)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.split in [\"test\", \"val\"]:\n",
        "            self.info[\"n_samples\"][self.split] = 4\n",
        "            return 4\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def montage(self, length=20, replace=False, save_folder=None):\n",
        "        n_sel = length * length\n",
        "        sel = np.random.choice(self.__len__(), size=n_sel, replace=replace)\n",
        "\n",
        "        montage_img = montage2d(\n",
        "            imgs=self.imgs, n_channels=self.info[\"n_channels\"], sel=sel\n",
        "        )\n",
        "\n",
        "        if save_folder is not None:\n",
        "            if not os.path.exists(save_folder):\n",
        "                os.makedirs(save_folder)\n",
        "            montage_img.save(\n",
        "                os.path.join(save_folder, f\"{self.flag}_{self.split}_montage.jpg\")\n",
        "            )\n",
        "\n",
        "        return montage_img\n",
        "\n",
        "\n",
        "class BiomedicalDataset(MedMNIST2D):\n",
        "    flag = FLAG"
      ],
      "metadata": {
        "id": "CeEDlvpgdFH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainDataset = BiomedicalDataset(\n",
        "    split=\"train\",\n",
        "    transform=image_transform,\n",
        "    download=True,\n",
        ")\n",
        "TestDataset = BiomedicalDataset(\n",
        "    split=\"test\",\n",
        "    transform=image_transform,\n",
        "    download=True,\n",
        ")\n",
        "ValDataset = BiomedicalDataset(\n",
        "    split=\"val\",\n",
        "    transform=image_transform,\n",
        ")"
      ],
      "metadata": {
        "id": "5ssmxSA1gzWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainDataset.cut()\n",
        "TestDataset.cut()\n",
        "ValDataset.cut()"
      ],
      "metadata": {
        "id": "YXuBjyQUpn_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(TrainDataset))\n",
        "print(len(TestDataset))\n",
        "print(len(ValDataset))"
      ],
      "metadata": {
        "id": "aX1vH_mfpZ8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Description**"
      ],
      "metadata": {
        "id": "WHq5wFak9maH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INFO[FLAG]['description']"
      ],
      "metadata": {
        "id": "K-Ms4nubxAGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainDataset.montage(15)"
      ],
      "metadata": {
        "id": "cq91y0vwudm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "M9bvhMihdFNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Utils**"
      ],
      "metadata": {
        "id": "MunmaGXF1Zvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AvgMeter(object):\n",
        "    def __init__(self, num=40):\n",
        "        self.num = num\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.scores = []\n",
        "\n",
        "    def update(self, val):\n",
        "        self.scores.append(val)\n",
        "\n",
        "    def show(self):\n",
        "        out = torch.mean(\n",
        "            torch.stack(\n",
        "                self.scores[np.maximum(len(self.scores)-self.num, 0):]\n",
        "            )\n",
        "        )\n",
        "        return out"
      ],
      "metadata": {
        "id": "5eiUwWfv8JO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Noise Scheduler**"
      ],
      "metadata": {
        "id": "OUP90Trjij2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NoiseScheduler(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        T=DIFFUSION_STEP,\n",
        "        beta_start=BETA_START,\n",
        "        beta_end=BETA_END,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.T = T\n",
        "        self.beta = torch.linspace(beta_start, beta_end, T).to(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "        self.alpha = 1. - self.beta\n",
        "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
        "        sqrt_one_minus_alpha_hat = torch.sqrt(\n",
        "            1 - self.alpha_hat[t]\n",
        "        ).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "        noise = torch.randn_like(x)\n",
        "        x_noisy = sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * noise\n",
        "\n",
        "        return x_noisy, noise"
      ],
      "metadata": {
        "id": "3vQ3zUzsBSlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FORWARD = NoiseScheduler"
      ],
      "metadata": {
        "id": "C1eyVzjbmpH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **U-Net**"
      ],
      "metadata": {
        "id": "uWcPMvglioOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, channels, size):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.size = size\n",
        "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
        "        self.ln = nn.LayerNorm([channels])\n",
        "        self.ff_self = nn.Sequential(\n",
        "            nn.LayerNorm([channels]),\n",
        "            nn.Linear(channels, channels),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(channels, channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
        "        x_ln = self.ln(x)\n",
        "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
        "        attention_value = attention_value + x\n",
        "        attention_value = self.ff_self(attention_value) + attention_value\n",
        "        return attention_value.swapaxes(2, 1).view(\n",
        "            -1, self.channels, self.size, self.size,\n",
        "        )"
      ],
      "metadata": {
        "id": "L7xY4IKJ6V_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels, out_channels, mid_channels=None, residual=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.residual = residual\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels, mid_channels, kernel_size=3, padding=1, bias=False,\n",
        "            ),\n",
        "            nn.GroupNorm(1, mid_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(\n",
        "                mid_channels, out_channels, kernel_size=3, padding=1, bias=False,\n",
        "            ),\n",
        "            nn.GroupNorm(1, out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.residual:\n",
        "            return F.gelu(x + self.double_conv(x))\n",
        "        else:\n",
        "            return self.double_conv(x)"
      ],
      "metadata": {
        "id": "a16gGmzg6xeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, in_channels, residual=True),\n",
        "            DoubleConv(in_channels, out_channels),\n",
        "        )\n",
        "\n",
        "        self.emb_layer = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_dim,\n",
        "                out_channels\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        x = self.maxpool_conv(x)\n",
        "        emb = self.emb_layer(t)[:, :, None, None].repeat(\n",
        "            1, 1, x.shape[-2], x.shape[-1],\n",
        "        )\n",
        "        return x + emb"
      ],
      "metadata": {
        "id": "lx_dnYM27UQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.Upsample(\n",
        "            scale_factor=2,\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=True,\n",
        "        )\n",
        "        self.conv = nn.Sequential(\n",
        "            DoubleConv(in_channels, in_channels, residual=True),\n",
        "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
        "        )\n",
        "\n",
        "        self.emb_layer = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(emb_dim, out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_x, t):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([skip_x, x], dim=1)\n",
        "        x = self.conv(x)\n",
        "        emb = self.emb_layer(t)[:, :, None, None].repeat(\n",
        "            1, 1, x.shape[-2], x.shape[-1],\n",
        "        )\n",
        "        return x + emb"
      ],
      "metadata": {
        "id": "WqPJsXut6WBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, time):\n",
        "        inv_freq = 1.0 / (\n",
        "            10000\n",
        "            ** (\n",
        "                torch.arange(0, self.channels, 2, device=self.device).float()\n",
        "                / self.channels\n",
        "            )\n",
        "        )\n",
        "        pos_enc_a = torch.sin(time.repeat(1, self.channels // 2) * inv_freq)\n",
        "        pos_enc_b = torch.cos(time.repeat(1, self.channels // 2) * inv_freq)\n",
        "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
        "        return pos_enc"
      ],
      "metadata": {
        "id": "eRMJnUn08IQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        c_in=3,\n",
        "        c_out=3,\n",
        "        time_dim=256 // SCALE_DOWN,\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_conv = DoubleConv(c_in, 64 // SCALE_DOWN)\n",
        "        self.down1 = Down(64 // SCALE_DOWN, 128 // SCALE_DOWN, time_dim)\n",
        "        self.sa1 = SelfAttention(128 // SCALE_DOWN, IMAGE_SIZE // 2)\n",
        "        self.down2 = Down(128 // SCALE_DOWN, 256 // SCALE_DOWN, time_dim)\n",
        "        self.sa2 = SelfAttention(256 // SCALE_DOWN, IMAGE_SIZE // 4)\n",
        "        self.down3 = Down(256 // SCALE_DOWN, 256 // SCALE_DOWN, time_dim)\n",
        "        self.sa3 = SelfAttention(256 // SCALE_DOWN, IMAGE_SIZE // 8)\n",
        "\n",
        "        self.bridge1 = DoubleConv(256 // SCALE_DOWN, 512 // SCALE_DOWN)\n",
        "        self.bridge2 = DoubleConv(512 // SCALE_DOWN, 512 // SCALE_DOWN)\n",
        "        self.bridge3 = DoubleConv(512 // SCALE_DOWN, 256 // SCALE_DOWN)\n",
        "\n",
        "        self.up1 = Up(512 // SCALE_DOWN, 128 // SCALE_DOWN, time_dim)\n",
        "        self.sa4 = SelfAttention(128 // SCALE_DOWN, IMAGE_SIZE // 4)\n",
        "        self.up2 = Up(256 // SCALE_DOWN, 64 // SCALE_DOWN, time_dim)\n",
        "        self.sa5 = SelfAttention(64 // SCALE_DOWN, IMAGE_SIZE // 2)\n",
        "        self.up3 = Up(128 // SCALE_DOWN, 64 // SCALE_DOWN, time_dim)\n",
        "        self.sa6 = SelfAttention(64 // SCALE_DOWN, IMAGE_SIZE)\n",
        "        self.out_conv = nn.Conv2d(64 // SCALE_DOWN, c_out, kernel_size=1)\n",
        "\n",
        "        self.pos_encoding = PositionalEmbedding(time_dim, device)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t = t.unsqueeze(-1).type(torch.float)\n",
        "        t = self.pos_encoding(t)\n",
        "\n",
        "        x1 = self.input_conv(x)\n",
        "        x2 = self.down1(x1, t)\n",
        "        x2 = self.sa1(x2)\n",
        "        x3 = self.down2(x2, t)\n",
        "        x3 = self.sa2(x3)\n",
        "        x4 = self.down3(x3, t)\n",
        "        x4 = self.sa3(x4)\n",
        "\n",
        "        x4 = self.bridge1(x4)\n",
        "        x4 = self.bridge2(x4)\n",
        "        x4 = self.bridge3(x4)\n",
        "\n",
        "        x = self.up1(x4, x3, t)\n",
        "        x = self.sa4(x)\n",
        "        x = self.up2(x, x2, t)\n",
        "        x = self.sa5(x)\n",
        "        x = self.up3(x, x1, t)\n",
        "        x = self.sa6(x)\n",
        "        output = self.out_conv(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "2UiU_7rO76Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BACKWARD = UNet"
      ],
      "metadata": {
        "id": "f8Y-sm8FmlrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Wrapper**"
      ],
      "metadata": {
        "id": "tn_nP8Uqmi6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionModel(L.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        forward_model,\n",
        "        backward_model,\n",
        "        batch_size,\n",
        "        lr,\n",
        "        max_epoch,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.forward_model = forward_model\n",
        "        self.backward_model = backward_model\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.max_epoch = max_epoch\n",
        "\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "        self.model_loss = []\n",
        "        self.val_fid = []\n",
        "\n",
        "        self.model_loss_recorder = AvgMeter()\n",
        "        self.val_fid_recorder = AvgMeter()\n",
        "\n",
        "        self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self._T = self.forward_model.T\n",
        "\n",
        "        self.fid = FrechetInceptionDistance(\n",
        "            feature=64,\n",
        "            input_img_size=(3, IMAGE_SIZE, IMAGE_SIZE),\n",
        "            normalize=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, x=None, t=None):\n",
        "        if self.training:\n",
        "            x_noisy, noise = self.forward_model(x, t)\n",
        "            noise_pred = self.backward_model(x_noisy, t)\n",
        "            return F.l1_loss(noise, noise_pred)\n",
        "        else:\n",
        "            return self.sample(progress=True, verbose=True)\n",
        "\n",
        "    def sample(self, n=1, progress=False, verbose=False, n_progress=5):\n",
        "        self.backward_model.eval()\n",
        "\n",
        "        progress_image = None if not progress else list()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = torch.randn(\n",
        "                (n, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
        "            ).to(self.device)\n",
        "\n",
        "            if progress:\n",
        "                progress_image.append(x.detach().cpu())\n",
        "\n",
        "            if verbose:\n",
        "                iteration = tqdm(reversed(range(1, self._T)), position=0)\n",
        "            else:\n",
        "                iteration = reversed(range(1, self._T))\n",
        "\n",
        "            for i in iteration:\n",
        "                t = (torch.ones(n) * i).long().to(self.device)\n",
        "\n",
        "                noise_pred = self.backward_model(x, t)\n",
        "                alpha = self.forward_model.alpha[t][:, None, None, None]\n",
        "                alpha_hat = self.forward_model.alpha_hat[t][:, None, None, None]\n",
        "                beta = self.forward_model.beta[t][:, None, None, None]\n",
        "\n",
        "                if i > 1:\n",
        "                    noise = torch.randn_like(x)\n",
        "                else:\n",
        "                    noise = torch.zeros_like(x)\n",
        "\n",
        "                x = (\n",
        "                    1 / torch.sqrt(alpha)\n",
        "                    * (\n",
        "                        x\n",
        "                        - ((1 - alpha) / (torch.sqrt(1 - alpha_hat)))\n",
        "                        * noise_pred\n",
        "                      )\n",
        "                    + torch.sqrt(beta)\n",
        "                    * noise\n",
        "                )\n",
        "                x = torch.clamp(x, -1.0, 1.0)\n",
        "\n",
        "                if (i+1) % (self._T//n_progress) == 0 and progress:\n",
        "                    progress_image.append(x.detach().cpu())\n",
        "\n",
        "        if progress:\n",
        "            progress_image.pop(1)\n",
        "\n",
        "            return progress_image\n",
        "\n",
        "        return x\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        self.fid.reset()\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        x = batch\n",
        "\n",
        "        self.fid.update((x + 1.0) / 2.0, real=True)\n",
        "\n",
        "        t = torch.randint(\n",
        "            0,\n",
        "            self._T,\n",
        "            (self.batch_size,),\n",
        "            device=self._device,\n",
        "        ).long()\n",
        "\n",
        "        loss = self(x, t)\n",
        "\n",
        "        opt = self.optimizers()\n",
        "        opt.zero_grad()\n",
        "        self.manual_backward(loss)\n",
        "        opt.step()\n",
        "\n",
        "        self.log(\"model_loss\", loss, prog_bar=True)\n",
        "        self.model_loss_recorder.update(loss.data)\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        self.model_loss.append(\n",
        "            self.model_loss_recorder.show().data.cpu().numpy(),\n",
        "        )\n",
        "        self.model_loss_recorder = AvgMeter()\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        x = batch\n",
        "        self.fid.update((x + 1.0) / 2.0, real=True)\n",
        "\n",
        "        _x = self.sample(x.shape[0])\n",
        "        self.fid.update((_x + 1.0) / 2.0, real=False)\n",
        "\n",
        "        fid = self.fid.compute().data.cpu()\n",
        "        self.log(\"val_fid\", fid, prog_bar=True)\n",
        "        self.val_fid_recorder.update(fid)\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        self.val_fid.append(self.val_fid_recorder.show().data.cpu().numpy())\n",
        "        self.val_fid_recorder = AvgMeter()\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        x = batch\n",
        "        self.fid.update((x + 1.0) / 2.0, real=True)\n",
        "\n",
        "        _x = self.sample(x.shape[0])\n",
        "        self.fid.update((_x + 1.0) / 2.0, real=False)\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        fid = self.fid.compute().data.cpu()\n",
        "        self.log(\"test_fid\", fid, prog_bar=False, logger=True)\n",
        "\n",
        "    def on_train_end(self):\n",
        "        # Loss\n",
        "        loss_img_file = f\"/content/{MODEL_NAME}_loss_plot.png\"\n",
        "        plt.plot(self.model_loss, color=\"r\")\n",
        "        plt.title(\"Loss Curves\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.grid()\n",
        "        plt.savefig(loss_img_file)\n",
        "        plt.clf()\n",
        "        img = cv2.imread(loss_img_file)\n",
        "        cv2_imshow(img)\n",
        "\n",
        "        # Evaluation Metrics\n",
        "        evaluation_metric_img_file = f\"/content/{MODEL_NAME}_fid_plot.png\"\n",
        "        plt.plot(self.val_fid[1:], color=\"b\")\n",
        "        plt.title(\"FID Curves\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"FID\")\n",
        "        plt.grid()\n",
        "        plt.savefig(evaluation_metric_img_file)\n",
        "        plt.clf()\n",
        "        img = cv2.imread(evaluation_metric_img_file)\n",
        "        cv2_imshow(img)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return data.DataLoader(\n",
        "            dataset=TrainDataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return data.DataLoader(\n",
        "            dataset=ValDataset,\n",
        "            batch_size=2,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return data.DataLoader(\n",
        "            dataset=TestDataset,\n",
        "            batch_size=2,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=self.lr)\n",
        "\n",
        "        return [optimizer]"
      ],
      "metadata": {
        "id": "tyKftij9milI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = DiffusionModel.__name__"
      ],
      "metadata": {
        "id": "9cngPQKTtyGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "boRJQCOJdFXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = int(np.random.randint(2147483647))\n",
        "print(f\"Random seed: {SEED}\")"
      ],
      "metadata": {
        "id": "J1S2oUivdFcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(SEED, workers=True)\n",
        "\n",
        "model = DiffusionModel(FORWARD(), BACKWARD(), BATCH_SIZE, LR, MAX_EPOCH)\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    monitor='val_fid',\n",
        "    dirpath=CHECKPOINT_DIR,\n",
        "    mode='min',\n",
        "    save_last=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    accelerator=\"auto\",\n",
        "    devices=1,\n",
        "    max_epochs=MAX_EPOCH,\n",
        "    callbacks=[checkpoint],\n",
        "    log_every_n_steps=5,\n",
        ")\n",
        "trainer.fit(model)"
      ],
      "metadata": {
        "id": "96vkBWERjAwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing**"
      ],
      "metadata": {
        "id": "rx7WK_KxdFhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.rename(\n",
        "    checkpoint.best_model_path,\n",
        "    os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}_best.ckpt\")\n",
        ")"
      ],
      "metadata": {
        "id": "-oAcMZsYdFmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test(ckpt_path=os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}_best.ckpt\"))"
      ],
      "metadata": {
        "id": "5IKKvIH3jEHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference**"
      ],
      "metadata": {
        "id": "JfvehFoAiW16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Utils**"
      ],
      "metadata": {
        "id": "9U0lYLLUR23C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_tensor_image(image):\n",
        "    reverse_transforms = Compose([\n",
        "        Lambda(lambda t: (t + 1) / 2),\n",
        "        Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "        Lambda(lambda t: t * 255.),\n",
        "        Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
        "        ToPILImage(),\n",
        "        Resize(IMAGE_SIZE),\n",
        "    ])\n",
        "\n",
        "    # Take first image of batch\n",
        "    if len(image.shape) == 4:\n",
        "        image = image[0, :, :, :]\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.imshow(reverse_transforms(image))"
      ],
      "metadata": {
        "id": "T_WGrCSbR49G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECKPOINT_INFERENCE = os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}_best.ckpt\")\n",
        "CHECKPOINT_INFERENCE = os.path.join(CHECKPOINT_DIR, f\"last.ckpt\")"
      ],
      "metadata": {
        "id": "ndWwtFfmJZ_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiffusionModel.load_from_checkpoint(\n",
        "    checkpoint_path=CHECKPOINT_INFERENCE,\n",
        "    map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    forward_model=FORWARD(),\n",
        "    backward_model=BACKWARD(),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    lr=LR,\n",
        "    max_epoch=MAX_EPOCH,\n",
        ")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "9FsMDqR7QOfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize**"
      ],
      "metadata": {
        "id": "ym6UVoVoR6NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "progress_image = model()\n",
        "for index, image in enumerate(progress_image):\n",
        "    plt.subplot(1, 5, index + 1)\n",
        "    show_tensor_image(image)"
      ],
      "metadata": {
        "id": "e9STip-Oh5sY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}